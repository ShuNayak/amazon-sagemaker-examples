{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host Your Pretrained Models on SageMaker in 3 Steps\n",
    "There are a lot of advantages to developing and training your models on SageMaker. (1) It's fast and easy, so you can iterate quickly to experiment with new packages and ideas, then deploy these to your customers. (2) You can version all of your experiments, then replicate them months down the road after you've forgotten the specifics. (3) You can decouple your development from your training environment, and scale these separately. This lets you optimize your compute and, more importantly, your time.\n",
    "\n",
    "But let's say you've got a deadline, and you don't want to re-think your training environment right now. You just want to deploy what you have, and with as little overhead as possible. This is the example for you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Put your pretrained models into `container/scripts/models`\n",
    "We're going to create a container around your pretrained models to simplify the experience of bringing a model trained outside of SageMaker. For this notebook, just make sure you have them placed in the `container/scripts/models` directory.\n",
    "\n",
    "Everything inside of `container/scripts` is getting copied into SageMaker. So if you have a file in the same directory as `container/scripts` on your notebook instance, it's going to show up in the same directory on the SageMaker endpoint cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl  transformer.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls container/scripts/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write your prediction code\n",
    "\n",
    "#### Add your requirements to the container\n",
    "First, let's get your framework and package needs defined. The Dockerfile we're using has quite a few packages already installed in your image; they're right here.\n",
    "```python3 \n",
    "nginx \n",
    "ca-certificates \n",
    "numpy==1.16.2 scipy==1.2.1 pandas flask gevent gunicorn```\n",
    "In the cell below, feel free to add any additional package requirements right here. This example will add them to the base Docker image for you. \n",
    "\n",
    "You can fully configure how this code works, so as long as you can install the software in your Docker container, you can run it on SageMaker. It does not matter which framework you are using, or what version that is. You just need to set that when you build the image with `container/Dockerfile`.\n",
    "\n",
    "This example is using sklearn, but you can modify this to point to whichever framework you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "scikit_learn==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the function `add_to_base_container` from the `utils.py` package available with this example. That will create a new Docker file around your package needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing your file requirements.txt\n",
      "Created your target file container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "from utils import add_to_base_container\n",
    "                \n",
    "add_to_base_container('requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add your model to the container\n",
    "Here you'll define how to pick up your model artifacts from disk.\n",
    "\n",
    "This example is assuming that you're providing both a `model` and a `transformer` object, which can take your new data and transform it to the correct specifications for your model. We'll pick up both the model and the transformer, and add them to the container. Here they get added to a `base_predictor.py` file to create the final `container/scripts/predictor.py` file that will run on the endpoint.\n",
    "\n",
    "Remember - SageMaker is going to run this script __from inside the `container/script` directory__. That means you want to point to model files referencing them from there. Your notebook is above that in the directory, so if you want to test your models in this notebook before writing the script, make sure to change the path in the `pickle.load` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_script.py\n",
    "import pickle\n",
    "\n",
    "model = pickle.load(open('models/model.pkl', 'rb'))\n",
    "transformer = pickle.load(open('models/transformer.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy it\n",
    "Here, just pass in the name of your script. If you're doing this iteratively to test new versions, you can pass in a new version name to track its progress.\n",
    "\n",
    "If you want to change `utils.py` you are welcome to! Just remember, every time you make a change there you need to restart the kernel for this notebook so it incorporates the latest changes. \n",
    "\n",
    "Make sure you're chaing `image_version` as your developing your solution. The SageMaker API likes to see unique model names, so we need to actually create a new model name everytime we want to point to a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the details from your script, my_script.py\n",
      "Adding to target file: container/scripts/predictor.py\n",
      "cd container && docker tag scripts 023375022819.dkr.ecr.us-east-2.amazonaws.com/scripts:shareable-6\n",
      "cd container && docker push 023375022819.dkr.ecr.us-east-2.amazonaws.com/scripts:shareable-6\n",
      "Deploying your endpoint: shareable-6-endpoint\n"
     ]
    }
   ],
   "source": [
    "from utils import deploy\n",
    "import os\n",
    "\n",
    "image_version = '6'\n",
    "\n",
    "model_base_name = 'shareable'\n",
    "\n",
    "deploy('my_script.py', image_version, model_base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test it\n",
    "Now, let's test your endpoint to make sure it's handling data the way you expect it to. You'll need to wait 7+ minutes to test it, because SageMaker is actually creating a live RESTful API around your model when we say `.deploy()`. Alternatively, you can point to an endpoint that already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shareable-4-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = '{}-{}-endpoint'.format(model_base_name, image_version)\n",
    "print (endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "pred_array = np.array([[ 1,  2],\n",
    "       [ 1,  4],\n",
    "       [ 1,  0],\n",
    "       [10,  2],\n",
    "       [10,  4],\n",
    "       [10,  0]])\n",
    "\n",
    "with open('test_data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(pred_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=csv_serializer(open('test_data.csv')))\n",
    "\n",
    "y_pred = response['Body'].read().decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
